<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.6: http://docutils.sourceforge.net/" />
<title>How to setup Percona replication manager (PRM)</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 5951 2009-05-18 18:03:10Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left{
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: left }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block {
  margin-left: 2em ;
  margin-right: 2em }

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="how-to-setup-percona-replication-manager-prm">
<h1 class="title">How to setup Percona replication manager (PRM)</h1>

<p>Author: Yves Trudeau, Percona</p>
<p>June 2012</p>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#overview" id="id2">Overview</a><ul>
<li><a class="reference internal" href="#corosync" id="id3">Corosync</a></li>
<li><a class="reference internal" href="#pacemaker" id="id4">Pacemaker</a></li>
<li><a class="reference internal" href="#mysql-resource-agent" id="id5">mysql resource agent</a></li>
<li><a class="reference internal" href="#mysql" id="id6">MySQL</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installing-the-packages" id="id7">Installing the packages</a><ul>
<li><a class="reference internal" href="#redhat-centos-6" id="id8">Redhat/Centos 6</a></li>
<li><a class="reference internal" href="#debian-ubuntu" id="id9">Debian/Ubuntu</a></li>
<li><a class="reference internal" href="#redhat-centos-5" id="id10">Redhat/Centos 5</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuring-corosync" id="id11">Configuring corosync</a><ul>
<li><a class="reference internal" href="#creating-the-cluster-authkey" id="id12">Creating the cluster Authkey</a></li>
<li><a class="reference internal" href="#creating-the-corosync-conf-file" id="id13">Creating the corosync.conf file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuring-pacemaker" id="id14">Configuring Pacemaker</a></li>
<li><a class="reference internal" href="#configuring-mysql" id="id15">Configuring MySQL</a><ul>
<li><a class="reference internal" href="#installation-of-mysql" id="id16">Installation of MySQL</a></li>
<li><a class="reference internal" href="#required-grants" id="id17">Required Grants</a></li>
<li><a class="reference internal" href="#setup-replication" id="id18">Setup replication</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pacemaker-configuration" id="id19">Pacemaker configuration</a><ul>
<li><a class="reference internal" href="#downloading-the-latest-mysql-ra" id="id20">Downloading the latest MySQL RA</a></li>
<li><a class="reference internal" href="#id1" id="id21">Configuring Pacemaker</a><ul>
<li><a class="reference internal" href="#cluster-attributes" id="id22">Cluster attributes</a></li>
<li><a class="reference internal" href="#ip-configuration-for-replication" id="id23">IP configuration for replication</a></li>
<li><a class="reference internal" href="#the-mysql-resource-primitive" id="id24">The MySQL resource primitive</a></li>
<li><a class="reference internal" href="#the-master-slave-clone-set" id="id25">The Master slave clone set</a></li>
<li><a class="reference internal" href="#the-vip-primitives" id="id26">The VIP primitives</a></li>
<li><a class="reference internal" href="#reader-vip-location-rules" id="id27">Reader VIP location rules</a></li>
<li><a class="reference internal" href="#writer-vip-rules" id="id28">Writer VIP rules</a></li>
<li><a class="reference internal" href="#all-together" id="id29">All together</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#useful-pacemaker-commands" id="id30">Useful Pacemaker commands</a><ul>
<li><a class="reference internal" href="#to-check-the-cluster-status" id="id31">To check the cluster status</a></li>
<li><a class="reference internal" href="#to-view-and-or-edit-the-configuration" id="id32">To view and/or edit the configuration</a></li>
<li><a class="reference internal" href="#to-change-a-node-status" id="id33">To change a node status</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing-failover" id="id34">Testing failover</a><ul>
<li><a class="reference internal" href="#basic-tests" id="id35">Basic tests</a><ul>
<li><a class="reference internal" href="#manual-failover" id="id36">Manual failover</a></li>
<li><a class="reference internal" href="#slave-lagging" id="id37">Slave lagging</a></li>
<li><a class="reference internal" href="#replication-broken" id="id38">Replication broken</a></li>
<li><a class="reference internal" href="#kill-of-mysql" id="id39">Kill of MySQL</a></li>
<li><a class="reference internal" href="#kill-of-mysql-no-restart" id="id40">Kill of MySQL no restart</a></li>
<li><a class="reference internal" href="#reboot-1-node" id="id41">Reboot 1 node</a></li>
<li><a class="reference internal" href="#reboot-all-node" id="id42">Reboot all node</a></li>
</ul>
</li>
<li><a class="reference internal" href="#stonith-tests" id="id43">Stonith tests</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to" id="id44">How to</a><ul>
<li><a class="reference internal" href="#how-to-add-a-new-node" id="id45">How to add a new node</a></li>
<li><a class="reference internal" href="#how-to-repair-replication" id="id46">How to repair replication</a></li>
<li><a class="reference internal" href="#how-to-exclude-a-node-from-the-master-role-or-less-likely-to-be" id="id47">How to exclude a node from the master role (or less likely to be)</a></li>
<li><a class="reference internal" href="#how-to-verify-why-a-reader-vip-is-not-on-a-slave" id="id48">How to verify why a reader VIP is not on a slave</a></li>
<li><a class="reference internal" href="#how-to-clean-up-error-in-pacemaker" id="id49">How to clean up error in pacemaker</a></li>
<li><a class="reference internal" href="#configuring-a-report-slave-with-a-dedicated-vip" id="id50">Configuring a report slave with a dedicated VIP</a></li>
<li><a class="reference internal" href="#enabling-trace-in-the-resource-agent" id="id51">Enabling trace in the resource agent</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-topics" id="id52">Advanced topics</a><ul>
<li><a class="reference internal" href="#vipless-cluster-cloud" id="id53">VIPless cluster (cloud)</a></li>
<li><a class="reference internal" href="#non-multicast-cluster-cloud" id="id54">Non-multicast cluster (cloud)</a></li>
<li><a class="reference internal" href="#stonith-devices" id="id55">Stonith devices</a></li>
<li><a class="reference internal" href="#using-heartbeat" id="id56">Using heartbeat</a></li>
<li><a class="reference internal" href="#performing-rolling-restarts-for-config-changes" id="id57">Performing rolling restarts for config changes</a></li>
<li><a class="reference internal" href="#backups-with-prm" id="id58">Backups with PRM</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="overview">
<h1><a class="toc-backref" href="#id2">Overview</a></h1>
<p>The solution we building is basically made of 4 components: Corosync, Pacemaker, the mysql resource agent and MySQL itself.</p>
<div class="section" id="corosync">
<h2><a class="toc-backref" href="#id3">Corosync</a></h2>
<p>Corosync handles the communication between the nodes.  It implements a cluster protocol called Totem and communicates over UDP (default port 5405).  By default it uses multicast but version 1.4.2 also supports unicast (udpu).  Pacemaker uses Corosync as a messaging service.  Corosync is not the only communication layer that can be used with Pacemaker heartbeat is another one although its usage is going down.</p>
<p>Pacemaker can also use the heartbeat communication stack.  The setup using heartbeat is covered in the advanced topics.</p>
</div>
<div class="section" id="pacemaker">
<h2><a class="toc-backref" href="#id4">Pacemaker</a></h2>
<p>Pacemaker is the heart of the solution, it the part managing where the logic is.  Pacemaker maintains a <em>cluster information base</em> <strong>cib</strong> that is a share xml databases between all the actives nodes.  The updates to the cib are send synchronously too all the nodes through Corosync.  Pacemaker has an amazingly rich set of configuration settings and features that allows very complex designs.  Without going into too much details, here are some of the features offered:</p>
<blockquote>
<ul class="simple">
<li>location rules: locating resources on nodes based on some criteria</li>
<li>colocating rules: colocating resources based on some criteria</li>
<li>clone set: a bunch of similar resource</li>
<li>master-slave clone set: a clone set with different level of members</li>
<li>a resource group: a group of resources forced to be together</li>
<li>ordering rules: in which order should some operation be performed</li>
<li>Attributes: kind of cluster wide variables, can be permanent or transient</li>
<li>Monitoring: resource can be monitored</li>
<li>Notification: resource can be notified of a cluster wide change</li>
</ul>
</blockquote>
<p>and many more.  The Pacemaker logic works with scores, the highest score wins.</p>
</div>
<div class="section" id="mysql-resource-agent">
<h2><a class="toc-backref" href="#id5">mysql resource agent</a></h2>
<p>In order to manage mysql and mysql replication, Pacemaker uses a resource agent which is a bash script.  The mysql resource agent bash script supports a set of calls like start, stop, monitor, promote, etc.  That allows Pacemaker to perform the required actions.</p>
</div>
<div class="section" id="mysql">
<h2><a class="toc-backref" href="#id6">MySQL</a></h2>
<p>The final service, the database.</p>
</div>
</div>
<div class="section" id="installing-the-packages">
<h1><a class="toc-backref" href="#id7">Installing the packages</a></h1>
<div class="section" id="redhat-centos-6">
<h2><a class="toc-backref" href="#id8">Redhat/Centos 6</a></h2>
<pre class="literal-block">
[root&#64;host-01 ~]# yum install pacemaker corosync
</pre>
<p>On Centos 6.2, this will install Pacemaker 1.1.6 and corosync 1.4.1.</p>
</div>
<div class="section" id="debian-ubuntu">
<h2><a class="toc-backref" href="#id9">Debian/Ubuntu</a></h2>
<pre class="literal-block">
[root&#64;host-01 ~]# apt-get install pacemaker corosync
</pre>
<p>On Debian Wheezy, this will install Pacemaker 1.1.6 and corosync 1.4.2</p>
</div>
<div class="section" id="redhat-centos-5">
<h2><a class="toc-backref" href="#id10">Redhat/Centos 5</a></h2>
<p>On older releases of RHEL/Centos, you have to install some external repos first:</p>
<pre class="literal-block">
[root&#64;host-01 ~]# wget http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm
[root&#64;host-01 ~]# rpm -Uvh epel-release-5-4.noarch.rpm
[root&#64;host-01 ~]# wget -O /etc/yum.repos.d/pacemaker.repo http://clusterlabs.org/rpm/epel-5/clusterlabs.repo
[root&#64;host-01 ~]# yum install pacemaker corosync
</pre>
<p>On RHEL 5.8, this will install Pacemaker 1.0.12 and corosync 1.2.7.</p>
</div>
</div>
<div class="section" id="configuring-corosync">
<h1><a class="toc-backref" href="#id11">Configuring corosync</a></h1>
<div class="section" id="creating-the-cluster-authkey">
<h2><a class="toc-backref" href="#id12">Creating the cluster Authkey</a></h2>
<p>On <strong>one</strong> of the host, run the following command:</p>
<pre class="literal-block">
[root&#64;host-01 ~]# cd /etc/corosync
[root&#64;host-01 corosync]# corosync-keygen
</pre>
<p>The key generator needs entropy, to speed up the key generation, I suggest you run commands in another session like <tt class="docutils literal">tar cvj / | md5sum &gt; /dev/null</tt> and similar.  The resulting file is <tt class="docutils literal">/etc/corosync/authkey</tt> and its access bytes are 0400 and owner root, group root.  Copy the authkey file to the other hosts of the cluster, same location, owner and rights.</p>
</div>
<div class="section" id="creating-the-corosync-conf-file">
<h2><a class="toc-backref" href="#id13">Creating the corosync.conf file</a></h2>
<p>The next step is to configure the communiction layer, corosync by creating the corosync configuration file <tt class="docutils literal">/etc/corosync/corosync.conf</tt>.  Let's consider the hosts in question have eth1 on the 172.30.222.x network.  A basic corosync configuration will look like:</p>
<pre class="literal-block">
compatibility: whitetank

totem {
      version: 2
      secauth: on
      threads: 0
      interface {
               ringnumber: 0
               bindnetaddr: 172.30.222.0
               mcastaddr: 226.94.1.1
               mcastport: 5405
               ttl: 1
      }
}

logging {
      fileline: off
      to_stderr: no
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
               subsys: AMF
               debug: off
      }
}

amf {
      mode: disabled
}
</pre>
<p>copy the file to both servers and start corosync with <tt class="docutils literal">service corosync start</tt>.  In order to verify corosync is working correctly, run the following command:</p>
<pre class="literal-block">
[root&#64;host-01 corosync]# corosync-objctl | grep members | grep ip
runtime.totem.pg.mrp.srp.members.-723640660.ip=r(0) ip(172.30.222.212)
runtime.totem.pg.mrp.srp.members.-1042407764.ip=r(0) ip(172.30.222.193)
</pre>
<p>This shows the 2 nodes that are member of the cluster.  If you have more than 2 nodes, you should have more similar entries. If you don't have an output similar to the above, make sure iptables is not blocking udp port 5405 and inspect the content of <tt class="docutils literal">/var/log/cluster/corosync.log</tt> for more information.</p>
<p>The above corosync configuration file is minimalist, it can be expanded in many ways.  For more information, <tt class="docutils literal">man corosync.conf</tt> is your friend.</p>
<p><strong>NOTE:</strong>  Older versions of corosync (RHEL/Centos 5) may not the members when running the <em>corosync-objctl</em> command.  You can see communication taking place with the following command (change the eth if not eth1):</p>
<pre class="literal-block">
tcpdump -i eth1 -n port 5405
</pre>
<p>And you should see output similar to the following:</p>
<pre class="literal-block">
09:57:46.969162 IP 172.30.222.212.hpoms-dps-lstn &gt; 172.30.222.193.netsupport: UDP, length 107
09:57:46.989108 IP 172.30.222.193.hpoms-dps-lstn &gt; 226.94.1.1.netsupport: UDP, length 119
09:57:47.159079 IP 172.30.222.193.hpoms-dps-lstn &gt; 172.30.222.212.netsupport: UDP, length 107
</pre>
</div>
</div>
<div class="section" id="configuring-pacemaker">
<h1><a class="toc-backref" href="#id14">Configuring Pacemaker</a></h1>
<p>The OS level configuration for Pacemaker is very simple, create the file <tt class="docutils literal">/etc/corosync/service.d/pacemaker</tt> with the following content:</p>
<pre class="literal-block">
service {
      name: pacemaker
      ver: 1
}
</pre>
<p>then, you can start pacemaker with <tt class="docutils literal">service pacemaker start</tt>.  Once started, you should be able to verify the cluster status with the crm command:</p>
<pre class="literal-block">
[root&#64;host-02 corosync]# crm status
============
Last updated: Thu May 24 17:06:57 2012
Last change: Thu May 24 17:05:32 2012 via crmd on host-01
Stack: openais
Current DC: host-01 - partition with quorum
Version: 1.1.6-3.el6-a02c0f19a00c1eb2527ad38f146ebc0834814558
2 Nodes configured, 2 expected votes
0 Resources configured.
============

Online: [ host-01 host-02 ]
</pre>
<p>Here, <tt class="docutils literal"><span class="pre">host-01</span></tt> and <tt class="docutils literal"><span class="pre">host-02</span></tt> correspond to the <tt class="docutils literal">uname <span class="pre">-n</span></tt> values.</p>
</div>
<div class="section" id="configuring-mysql">
<h1><a class="toc-backref" href="#id15">Configuring MySQL</a></h1>
<div class="section" id="installation-of-mysql">
<h2><a class="toc-backref" href="#id16">Installation of MySQL</a></h2>
<p>Install packages like you would normally do depending on the distribution you are using.  The minimal requirements for my.cnf are a unique <tt class="docutils literal">server_id</tt> for replication, <tt class="docutils literal"><span class="pre">log-bin</span></tt> to activate the binary log and <strong>not</strong> <tt class="docutils literal"><span class="pre">log-slave-updates</span></tt> since this screw up the logic.  Also, make sure pid-file and socket correspond to what will be defined below for the configuration of the mysql primitive in Pacemaker.  In our example, on Centos 6 servers:</p>
<pre class="literal-block">
[root&#64;host-01 ~]# cat /etc/my.cnf
[client]
socket=/var/run/mysqld/mysqld.sock
[mysqld]
datadir=/var/lib/mysql
socket=/var/run/mysqld/mysqld.sock
user=mysql
# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0
log-bin
server-id=1
pid-file=/var/lib/mysql/mysqld.pid
</pre>
<p>Start Mysql manually with <tt class="docutils literal">service mysql start</tt> or the equivalent.</p>
</div>
<div class="section" id="required-grants">
<h2><a class="toc-backref" href="#id17">Required Grants</a></h2>
<p>The following grants are needed:</p>
<pre class="literal-block">
grant replication client, replication slave on *.* to repl_user&#64;'172.30.222.%' identified by 'ola5P1ZMU';
grant replication client, replication slave, SUPER, PROCESS, RELOAD on *.* to repl_user&#64;'localhost' identified by 'ola5P1ZMU';
grant select ON mysql.user to test_user&#64;'localhost' identified by '2JcXCxKF';
</pre>
</div>
<div class="section" id="setup-replication">
<h2><a class="toc-backref" href="#id18">Setup replication</a></h2>
<p>You setup the replication like you normally do, make sure replication works fine between all hosts.  With 2 hosts, a good way of checking is to setup master-master replication.  Keep in mind though that PRM will only use master-slave.  Once done, stop MySQL and make sure it doesn't start automatically after boot.  In the future, Pacemaker will be managing MySQL</p>
</div>
</div>
<div class="section" id="pacemaker-configuration">
<h1><a class="toc-backref" href="#id19">Pacemaker configuration</a></h1>
<div class="section" id="downloading-the-latest-mysql-ra">
<h2><a class="toc-backref" href="#id20">Downloading the latest MySQL RA</a></h2>
<p>The PRM solution requires a specific Pacemaker MySQL resource agent.  The new resource agent is available in version 3.9.3 of the resource-agents package.  In the Centos version used for this documentation, the version of this package is:</p>
<pre class="literal-block">
[root&#64;host-01 corosync]# rpm -qa | grep resour
resource-agents-3.9.2-7.el6.i686
</pre>
<p>which will not do.  Since it is very recent, we can just download the latest agent from github like here:</p>
<pre class="literal-block">
[root&#64;host-01 corosync]# cd /usr/lib/ocf/resource.d/
[root&#64;host-01 resource.d]# mkdir percona
[root&#64;host-01 resource.d]# cd percona/
[root&#64;host-01 percona]# wget -q https://github.com/y-trudeau/resource-agents-prm/raw/master/heartbeat/mysql
[root&#64;host-01 percona]# chmod u+x mysql
</pre>
<p>The procedure must be repeated on all hosts.  We have created a &quot;percona&quot; directory to make sure there would be no conflict with the default MySQL resource agent if the resource-agents package is updated.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id21">Configuring Pacemaker</a></h2>
<div class="section" id="cluster-attributes">
<h3><a class="toc-backref" href="#id22">Cluster attributes</a></h3>
<p>For the sake of simplicity we start by a 2 nodes cluster.  The problem with a 2 nodes cluster is the loss of quorum as soon as one of the hosts is down.  In order to have a functional 2 nodes we must set the <em>no-quorum-policy</em> to ignore like this:</p>
<pre class="literal-block">
crm_attribute --attr-name no-quorum-policy --attr-value ignore
</pre>
<p>This can be revisited for larger clusters.  Also, since for this example we are not configuring any stonith devices, we have to disable stonith with:</p>
<pre class="literal-block">
crm_attribute --attr-name stonith-enabled --attr-value false
</pre>
</div>
<div class="section" id="ip-configuration-for-replication">
<h3><a class="toc-backref" href="#id23">IP configuration for replication</a></h3>
<p>The PRM solution needs to know which IP it should use to connect to a master when configuring replication, basically, for the <em>master_host</em> parameter of the <tt class="docutils literal">change master to</tt> command.  There's 2 ways of configuring the IPs.</p>
<p>The default way is to make sure the host names resolves correctly on all the members of the cluster.  Collect the hostnames with <tt class="docutils literal">uname <span class="pre">-n</span></tt> and verify those names resolve to the IPs you want to from all hosts using replication.  If possible, avoid DNS and use /etc/hosts since DNS adds a big single point of failure.</p>
<p>The other way uses a node attribute.  For example, if the MySQL resource primitive name (next section) is <tt class="docutils literal">p_mysql</tt> then you can add <tt class="docutils literal">p_mysql_mysql_master_IP</tt> (<tt class="docutils literal">_mysql_master_IP</tt> concatenated to the resource name) to each node with the IP you want to use. Here's an example:</p>
<pre class="literal-block">
node host-01 \
      attributes p_mysql_mysql_master_IP=&quot;172.30.222.193&quot;
node host-02 \
      attributes p_mysql_mysql_master_IP=&quot;172.30.222.212&quot;
</pre>
<p>Which means the IP 172.30.222.193 will be use for the <tt class="docutils literal">change master to</tt> command when host-01 is the master and same for 172.30.222.212, which will be used when host-02 is the master.  These IPs correspond to the private network (eth1) of those hosts.  The best way to modify the Pacemaker configuration is with the command <tt class="docutils literal">crm configure edit</tt> which loads the configuration in vi.  Once done editing, save the file &quot;:wq&quot; and the new configuration will be loaded by Pacemaker.</p>
<p><strong>NOTE:</strong> Older versions of corosync (RHEL/Centos 5) may trigger an error like the following:</p>
<pre class="literal-block">
/var/run/crm/cib-invalid.vlD2Dq:14: element instance_attributes: Relax-NG validity error : Type ID doesn't allow value 'host-01-instance_attributes'
/var/run/crm/cib-invalid.vlD2Dq:14: element instance_attributes: Relax-NG validity error : Element instance_attributes failed to validate content
...
</pre>
<p>In this case, <tt class="docutils literal">vi</tt> many not work for attribute editing so you can use a command like the following to set the IP (or other attributes):</p>
<pre class="literal-block">
crm_attribute -l forever -G --node host-01 --name p_mysql_mysql_master_IP -v &quot;172.30.222.193&quot;
</pre>
</div>
<div class="section" id="the-mysql-resource-primitive">
<h3><a class="toc-backref" href="#id24">The MySQL resource primitive</a></h3>
<p>We are now ready to start giving work to Pacemaker the first thing we will do is configure the mysql primitive which defines how Pacemaker will call the mysql resource agent.  The resource has many parameter, let's first review them, the defautls presented are the ones for Linux.</p>
<table border="1" class="docutils">
<colgroup>
<col width="18%" />
<col width="82%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Parameter</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>binary</td>
<td>Location of the MySQL server binary. Typically, this will point to the mysqld or the mysqld_safe file.
The recommended value is the the path of the the mysqld binary, be aware it may not be the defautl.
<em>default: /usr/bin/safe_mysqld</em></td>
</tr>
<tr><td>client_binary</td>
<td>Location of the MySQL client binary.  <em>default: mysql</em></td>
</tr>
<tr><td>config</td>
<td>Location of the mysql configuation file. <em>default: /etc/my.cnf</em></td>
</tr>
<tr><td>datadir</td>
<td>Directory containing the MySQL database <em>default: /var/lib/mysql</em></td>
</tr>
<tr><td>user</td>
<td>Unix user under which will run the MySQL daemon <em>default: mysql</em></td>
</tr>
<tr><td>group</td>
<td>Unix group under which will run the MySQL daemon <em>default: mysql</em></td>
</tr>
<tr><td>log</td>
<td>The logfile to be used for mysqld. <em>default: /var/log/mysqld.log</em></td>
</tr>
<tr><td>pid</td>
<td>The location of the pid file for mysqld process. <em>default: /var/run/mysql/mysqld.pid</em></td>
</tr>
<tr><td>socket</td>
<td>The MySQL Unix socket file. <em>default: /var/lib/mysql/mysql.sock</em></td>
</tr>
<tr><td>test_table</td>
<td>The table used to test mysql with a <tt class="docutils literal">select <span class="pre">count(*)</span></tt>. <em>default: mysql.user</em></td>
</tr>
<tr><td>test_user</td>
<td>The MySQL user performing the test on the test table.  Must have <tt class="docutils literal">grant select</tt> on the test table.
<em>default: root</em></td>
</tr>
<tr><td>test_passwd</td>
<td>Password of the test user. <em>default: no set</em></td>
</tr>
<tr><td>enable_creation</td>
<td>Runs <tt class="docutils literal">mysql_install_db</tt> if the datadir is not configured. <em>default: 0 (boolean 0 or 1)</em></td>
</tr>
<tr><td>additional_parameters</td>
<td>Additional MySQL parameters passed (example <tt class="docutils literal"><span class="pre">--skip-grant-tables</span></tt>). <em>default: no set</em></td>
</tr>
<tr><td>replication_user</td>
<td>The MySQL user to use in the <tt class="docutils literal">change master to master_user</tt> command.  The user must have
REPLICATION SLAVE and REPLICATION CLIENT from the other hosts and SUPER, REPLICATION SLAVE,
REPLICATION CLIENT, and PROCESS from localhost.  <em>default: no set</em></td>
</tr>
<tr><td>replication_passwd</td>
<td>The password of the replication_user. <em>default: no set</em></td>
</tr>
<tr><td>replication_port</td>
<td>TCP Port to use for MySQL replication. <em>default: 3306</em></td>
</tr>
<tr><td>max_slave_lag</td>
<td>The maximum number of seconds a replication slave is allowed to lag behind its master.
Do not set this to zero. What the cluster manager does in case a slave exceeds this maximum lag
is determined by the evict_outdated_slaves parameter.  If evict_outdated_slaves is true, slave is
stopped and if false, only a transcient attribute (see reader_attribute) is set to 0.</td>
</tr>
<tr><td>evict_outdated_slaves</td>
<td>This parameter instructs the resource agent how to react if the slave is lagging behind by more
than max_slave_lag.  When set to true, outdated slaves are stopped.  <em>default: false</em></td>
</tr>
<tr><td>reader_attribute</td>
<td>This parameter sets the name of the transient attribute that can be used to adjust the behavior
of the cluster given the state of the slave.  Each slaves updates this attributor at each
monitor call and sets it to 1 is sane and 0 if not sane.  Sane is defined as lagging by less than
max_slave_lag and slave threads are running.  <em>default: readable</em></td>
</tr>
<tr><td>reader_failcount</td>
<td>The number of times a monitor operation can find the slave to be unsuitable for reader VIP
before failing.  Useful if there are short intermittent issues like clock adjustments in VMs.</td>
</tr>
</tbody>
</table>
<p>So here's a typical primitive declaration:</p>
<pre class="literal-block">
primitive p_mysql ocf:percona:mysql \
      params config=&quot;/etc/my.cnf&quot; pid=&quot;/var/lib/mysql/mysqld.pid&quot; socket=&quot;/var/run/mysqld/mysqld.sock&quot; replication_user=&quot;repl_user&quot; \
             replication_passwd=&quot;ola5P1ZMU&quot; max_slave_lag=&quot;60&quot; evict_outdated_slaves=&quot;false&quot; binary=&quot;/usr/libexec/mysqld&quot; \
             test_user=&quot;test_user&quot; test_passwd=&quot;2JcXCxKF&quot; \
      op monitor interval=&quot;5s&quot; role=&quot;Master&quot; OCF_CHECK_LEVEL=&quot;1&quot; \
      op monitor interval=&quot;2s&quot; role=&quot;Slave&quot; OCF_CHECK_LEVEL=&quot;1&quot; \
      op start interval=&quot;0&quot; timeout=&quot;60s&quot; \
      op stop interval=&quot;0&quot; timeout=&quot;60s&quot;
</pre>
<p>An easy way to load the above fragment is to use the <tt class="docutils literal">crm configure edit</tt> command.  You will notice that we also define two monitor operations, one for the role Master and one for role slave with different intervals.  It is important to have different intervals, for Pacemaker internal reasons. Also, I defined the timeout for start and stop to 60s, make sure you have configured innodb_log_file_size in a way that mysql can stop in less than 60s with the maximum allowed number of dirty pages and that it can start in less than 60s while having to perform Innodb recovery.  Since the snippet refers to role Master and Slave, you need to also include the master slave clone set (below).</p>
</div>
<div class="section" id="the-master-slave-clone-set">
<h3><a class="toc-backref" href="#id25">The Master slave clone set</a></h3>
<p>Next we need to tell Pacemaker to start a set of similar resource (the p_mysql type primitive) and consider the primitives in the set as having 2 states, Master and slave.  This type of declaration uses the <tt class="docutils literal">ms</tt> type (for master-slave).  The configuration snippet for the <tt class="docutils literal">ms</tt> is:</p>
<pre class="literal-block">
ms ms_MySQL p_mysql \
     meta master-max=&quot;1&quot; master-node-max=&quot;1&quot; clone-max=&quot;2&quot; clone-node-max=&quot;1&quot; notify=&quot;true&quot; globally-unique=&quot;false&quot; target-role=&quot;Master&quot; is-managed=&quot;true&quot;
</pre>
<p>Here, the importants elements are clone-max and notify.  <tt class="docutils literal"><span class="pre">clone-max</span></tt> is the number of databases node involded in the <tt class="docutils literal">ms</tt> set.  Since we are consider a two nodes cluster, it is set to 2.  If we ever add a node, we will need to increase <tt class="docutils literal"><span class="pre">clone-max</span></tt> to 3.  The solution works with notification, so it is mandatory to enable notifications with <tt class="docutils literal">notify</tt> set to true.</p>
</div>
<div class="section" id="the-vip-primitives">
<h3><a class="toc-backref" href="#id26">The VIP primitives</a></h3>
<p>Let's assume we want to have a writer virtual IP (VIP), 172.30.222.100 and two reader virtual IPs, 172.30.222.101 and 172.30.222.102.  The first thing we need to do is to add the primitives to the cluster configuration.  Those primitives will look like:</p>
<pre class="literal-block">
primitive reader_vip_1 ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.101&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
primitive reader_vip_2 ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.102&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
primitive writer_vip ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.100&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
</pre>
<p>After adding these primitives to the cluster configuration with <tt class="docutils literal">crm configure edit</tt>, the VIPs will be distributed in a round-robin fashion, not exactly ideal.  This is why we need to add rules to control on which hosts they'll be on.</p>
</div>
<div class="section" id="reader-vip-location-rules">
<h3><a class="toc-backref" href="#id27">Reader VIP location rules</a></h3>
<p>One of the new element introduced with this solution is the addition of a transient attribute to control if a host is suitable to host a reader VIP.  The replication master are always suitable but the slave suitability is determine by the monitor operation which set the transient attribute to 1 is ok and to 0 is not.  In the MySQL primitive above, we have not set the <em>reader_attribute</em> parameter so we are using the default value &quot;readable&quot; for the transient attribute.  The use of the transient attribute is through a location rule which will but a score on -infinity for the VIPs to be located on unsuitable hosts.  The location rules for the reader VIPs are the following:</p>
<pre class="literal-block">
location loc-no-reader-vip-1 reader_vip_1 \
      rule $id=&quot;rule-no-reader-vip-1&quot; -inf: readable eq 0
location loc-No-reader-vip-2 reader_vip_2 \
      rule $id=&quot;rule-no-reader-vip-2&quot; -inf: readable eq 0
</pre>
<p>Again, use <tt class="docutils literal">crm configure edit</tt> to add the these rules.</p>
</div>
<div class="section" id="writer-vip-rules">
<h3><a class="toc-backref" href="#id28">Writer VIP rules</a></h3>
<p>The writer VIP is simpler, it is bound to the master.  This is achieved with a colocation rule and an order like below:</p>
<pre class="literal-block">
colocation writer_vip_on_master inf: writer_vip ms_MySQL:Master
order ms_MySQL_promote_before_vip inf: ms_MySQL:promote writer_vip:start
</pre>
</div>
<div class="section" id="all-together">
<h3><a class="toc-backref" href="#id29">All together</a></h3>
<p>Here's all the snippets grouped together:</p>
<pre class="literal-block">
[root&#64;host-01 ~]# crm configure show
node host-01 \
      attributes p_mysql_mysql_master_IP=&quot;172.30.222.193&quot;
node host-02 \
      attributes p_mysql_mysql_master_IP=&quot;172.30.222.212&quot;
primitive p_mysql ocf:percona:mysql \
      params config=&quot;/etc/my.cnf&quot; pid=&quot;/var/lib/mysql/mysqld.pid&quot; socket=&quot;/var/run/mysqld/mysqld.sock&quot; replication_user=&quot;repl_user&quot; replication_passwd=&quot;ola5P1ZMU&quot; max_slave_lag=&quot;60&quot; evict_outdated_slaves=&quot;false&quot; binary=&quot;/usr/libexec/mysqld&quot; test_user=&quot;test_user&quot; test_passwd=&quot;2JcXCxKF&quot; \
      op monitor interval=&quot;5s&quot; role=&quot;Master&quot; OCF_CHECK_LEVEL=&quot;1&quot; \
      op monitor interval=&quot;2s&quot; role=&quot;Slave&quot; OCF_CHECK_LEVEL=&quot;1&quot; \
      op start interval=&quot;0&quot; timeout=&quot;60s&quot; \
      op stop interval=&quot;0&quot; timeout=&quot;60s&quot;
primitive reader_vip_1 ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.101&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
primitive reader_vip_2 ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.102&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
primitive writer_vip ocf:heartbeat:IPaddr2 \
      params ip=&quot;172.30.222.100&quot; nic=&quot;eth1&quot; \
      op monitor interval=&quot;10s&quot;
ms ms_MySQL p_mysql \
      meta master-max=&quot;1&quot; master-node-max=&quot;1&quot; clone-max=&quot;2&quot; clone-node-max=&quot;1&quot; notify=&quot;true&quot; globally-unique=&quot;false&quot; target-role=&quot;Master&quot; is-managed=&quot;true&quot;
location loc-No-reader-vip-2 reader_vip_2 \
      rule $id=&quot;rule-no-reader-vip-2&quot; -inf: readable eq 0
location loc-no-reader-vip-1 reader_vip_1 \
      rule $id=&quot;rule-no-reader-vip-1&quot; -inf: readable eq 0
colocation writer_vip_on_master inf: writer_vip ms_MySQL:Master
order ms_MySQL_promote_before_vip inf: ms_MySQL:promote writer_vip:start
property $id=&quot;cib-bootstrap-options&quot; \
      dc-version=&quot;1.1.6-3.el6-a02c0f19a00c1eb2527ad38f146ebc0834814558&quot; \
      cluster-infrastructure=&quot;openais&quot; \
      expected-quorum-votes=&quot;2&quot; \
      no-quorum-policy=&quot;ignore&quot; \
      stonith-enabled=&quot;false&quot; \
      last-lrm-refresh=&quot;1338928815&quot;
property $id=&quot;mysql_replication&quot; \
      p_mysql_REPL_INFO=&quot;172.30.222.193|mysqld-bin.000002|106&quot;
</pre>
<p>You'll notice toward the end, the <tt class="docutils literal">p_mysql_REPL_INFO</tt> attribute (the value may differ) that correspond to the master status when it has been promoted to master.</p>
</div>
</div>
</div>
<div class="section" id="useful-pacemaker-commands">
<h1><a class="toc-backref" href="#id30">Useful Pacemaker commands</a></h1>
<div class="section" id="to-check-the-cluster-status">
<h2><a class="toc-backref" href="#id31">To check the cluster status</a></h2>
<p>Two tools can be used to query the cluster status, <tt class="docutils literal">crm_mon</tt> and <tt class="docutils literal">crm status</tt>.  They produce the same output but <tt class="docutils literal">crm_mon</tt> is more like top, it stays on screen and refreshes at every changes.  <tt class="docutils literal">crm status</tt> is a one time status dump.  The output is the following:</p>
<pre class="literal-block">
[root&#64;host-01 ~]# crm status
============
Last updated: Tue Jun  5 17:09:01 2012
Last change: Tue Jun  5 16:43:08 2012 via cibadmin on host-01
Stack: openais
Current DC: host-01 - partition with quorum
Version: 1.1.6-3.el6-a02c0f19a00c1eb2527ad38f146ebc0834814558
2 Nodes configured, 2 expected votes
5 Resources configured.
============

Online: [ host-01 host-02 ]

Master/Slave Set: ms_MySQL [p_mysql]
   Masters: [ host-01 ]
   Slaves: [ host-02 ]
reader_vip_1   (ocf::heartbeat:IPaddr2):       Started host-01
reader_vip_2   (ocf::heartbeat:IPaddr2):       Started host-02
writer_vip     (ocf::heartbeat:IPaddr2):       Started host-01
</pre>
</div>
<div class="section" id="to-view-and-or-edit-the-configuration">
<h2><a class="toc-backref" href="#id32">To view and/or edit the configuration</a></h2>
<p>To view the current configuration use <tt class="docutils literal">crm configure show</tt> and to edit, use <tt class="docutils literal">crm configure edit</tt>.  The later command starts the vi editor on the current configuration.  If you want to use another editor, set the EDITOR session variable.</p>
</div>
<div class="section" id="to-change-a-node-status">
<h2><a class="toc-backref" href="#id33">To change a node status</a></h2>
<p>It is often required to put a node in standby mode in order to perform maintenance operations on it.  The best way is to use the <tt class="docutils literal">standby</tt> node status.  Let's consider this initial state:</p>
<pre class="literal-block">
root&#64;host-02:~# crm status
============
Last updated: Fri Nov 23 09:17:31 2012
Last change: Fri Nov 23 09:16:40 2012 via crm_attribute on host-01
Stack: openais
Current DC: host-01 - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
2 Nodes configured, 2 expected votes
5 Resources configured.
============

Online: [ host-01 host-02 ]

Master/Slave Set: ms_MySQL [p_mysql]
   Masters: [ host-01 ]
   Slaves: [ host-02 ]
reader_vip_1   (ocf::heartbeat:IPaddr2):       Started host-02
reader_vip_2   (ocf::heartbeat:IPaddr2):       Started host-01
writer_vip     (ocf::heartbeat:IPaddr2):       Started host-01
</pre>
<p>Now, if we want to put host-02 in standby we do <tt class="docutils literal">crm node standby <span class="pre">host-02</span></tt>, which, after a few seconds will produce the status:</p>
<pre class="literal-block">
root&#64;host-02:~# crm status
============
Last updated: Fri Nov 23 09:25:21 2012
Last change: Fri Nov 23 09:25:11 2012 via crm_attribute on host-02
Stack: openais
Current DC: host-01 - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
2 Nodes configured, 2 expected votes
5 Resources configured.
============

Node host-02: standby
Online: [ host-01 ]

Master/Slave Set: ms_MySQL [p_mysql]
   Masters: [ host-01 ]
   Stopped: [ p_mysql:1 ]
reader_vip_1   (ocf::heartbeat:IPaddr2):       Started host-01
reader_vip_2   (ocf::heartbeat:IPaddr2):       Started host-01
writer_vip     (ocf::heartbeat:IPaddr2):       Started host-01
</pre>
<p>The node host-02 can be put back online with <tt class="docutils literal">crm node online <span class="pre">host-02</span></tt>.  If above we would have chose to put host-01 in standby, the master role would have been switch to host-02 and the result would have been pretty similar, inverting host-01 and host-02 and the above status.</p>
</div>
</div>
<div class="section" id="testing-failover">
<h1><a class="toc-backref" href="#id34">Testing failover</a></h1>
<p>An HA setup is only HA in theory until tested so that's why the testing part is so important.</p>
<div class="section" id="basic-tests">
<h2><a class="toc-backref" href="#id35">Basic tests</a></h2>
<p>The basic tests don't require the presence of a stonith device and the minimalistic set of tests that should be performed.  All these tests should be run while sending writes to the master.  As a bare minimum, use simple bash script like:</p>
<pre class="literal-block">
#!/bin/bash
#
MYSQLCRED='-u writeuser -pwrites -h 172.30.212.100'

mysql $MYSQLCRED -e &quot;create database if not exists test;&quot;
mysql $MYSQLCRED -e &quot;create table if not exists writeload (id int not null auto_increment,data char(10), primary key (id)) engine = innodb;&quot; test

while [ 1 ]
do
   mysql $MYSQLCRED -e &quot;insert into writeload values (data) values ('test');&quot; test
   sleep 1
done
</pre>
<p>Adjust the credentials so that the writes can follow the writer VIP as it moves between servers.  Make sure you don't grant <tt class="docutils literal">SUPER</tt> since it breaks the read-only barrier.</p>
<div class="section" id="manual-failover">
<h3><a class="toc-backref" href="#id36">Manual failover</a></h3>
<p>If the master is host-01, but it in standby with <tt class="docutils literal">crm node standby <span class="pre">host-01</span></tt> and check that the inserts resume on the host-02.  The script may have thrown a few errors but that's normal.  Then, put host-01 back online with <tt class="docutils literal">crm node online <span class="pre">host-01</span></tt>, it should be back as a slave and should pickup the missing from replication.  Verify that replication is ok and there are no holes in the ids.</p>
</div>
<div class="section" id="slave-lagging">
<h3><a class="toc-backref" href="#id37">Slave lagging</a></h3>
<p>The following test is design to verify the behavior of the reader_vips when replication is lagging.  With the above write script still running, run the following query on the master:</p>
<pre class="literal-block">
insert into test.writeload select sleep(2*max_slave_lag);
</pre>
<p>For that to run, max_slave_lag must be larger than the monitor operation interval times the failcount for the slave in the <tt class="docutils literal">p_mysql</tt> primitive definition.  After you started the query on the master, start the shell tool <tt class="docutils literal">crm_mon</tt>.  After about 3 times the max_slave_lag, the reader_vip should move away from the slave and then after about 4 times max_slave_lag, go back.</p>
</div>
<div class="section" id="replication-broken">
<h3><a class="toc-backref" href="#id38">Replication broken</a></h3>
<p>If you break replication by inserting a row on the save in the writeload table, the reader_vip should move away from the affected slave in around the monitor operation interval times the failcount.  Once corrected, the reader_vip should come back.</p>
</div>
<div class="section" id="kill-of-mysql">
<h3><a class="toc-backref" href="#id39">Kill of MySQL</a></h3>
<p>A kill of the <tt class="docutils literal">mysqld</tt> process, on either the master or the slave should cause Pacemaker to restart it.  If the restart are normal, there's no need for the master role to switch over.</p>
</div>
<div class="section" id="kill-of-mysql-no-restart">
<h3><a class="toc-backref" href="#id40">Kill of MySQL no restart</a></h3>
<p>As we are progressing in our tests, let's be a bit rougher with MySQL, we'll kill the master mysqld process but we will start nc to bind the 3306 port, preventing it to restart.  It is advisable to reduce the <tt class="docutils literal">op start</tt> and <tt class="docutils literal">op stop</tt> values for that test, 900s is a long while to wait.  I personally ran the test with both at 20s.  So, on the master, run:</p>
<pre class="literal-block">
kill `pidof mysqld`; nc -l -p 3306 &gt; /dev/null &amp;
</pre>
<p>In my case, the master was host-02.  After a short while the status should be like:</p>
<pre class="literal-block">
root&#64;host-02:~# crm status
============
Last updated: Fri Nov 23 13:55:55 2012
Last change: Fri Nov 23 13:53:06 2012 via crm_attribute on host-01
Stack: openais
Current DC: host-01 - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
2 Nodes configured, 2 expected votes
5 Resources configured.
============

Online: [ host-01 host-02 ]

Master/Slave Set: ms_MySQL [p_mysql]
   Masters: [ host-01 ]
   Stopped: [ p_mysql:1 ]
reader_vip_1   (ocf::heartbeat:IPaddr2):       Started host-01
reader_vip_2   (ocf::heartbeat:IPaddr2):       Started host-01
writer_vip     (ocf::heartbeat:IPaddr2):       Started host-01

Failed actions:
   p_mysql:1_start_0 (node=host-02, call=87, rc=-2, status=Timed Out): unknown exec error
</pre>
<p>If another node is promoted master than test is successful.  To put thing back in place do the following step on the failed node:</p>
<pre class="literal-block">
root&#64;host-02:~# kill `pidof nc`; crm resource cleanup p_mysql:1

Cleaning up p_mysql:1 on host-01
Cleaning up p_mysql:1 on host-02
Waiting for 3 replies from the CRMd... OK
[1]+  Exit 1                  nc -l -p 3306 &gt; /dev/null
root&#64;host-02:~#
</pre>
<p>and host-02 should become a slave of host-01.</p>
</div>
<div class="section" id="reboot-1-node">
<h3><a class="toc-backref" href="#id41">Reboot 1 node</a></h3>
<p>Rebooting any of the nodes should always leave the database system with a master.  Be careful if you reboot nodes in sequences while writing to them, give at least a few seconds for the slave process to catch up.</p>
</div>
<div class="section" id="reboot-all-node">
<h3><a class="toc-backref" href="#id42">Reboot all node</a></h3>
<p>After the reboot, a master should be promoted and the other nodes should be slaves of the master.</p>
</div>
</div>
<div class="section" id="stonith-tests">
<h2><a class="toc-backref" href="#id43">Stonith tests</a></h2>
<p>For the following test, you need stonith devices defined.</p>
</div>
</div>
<div class="section" id="how-to">
<h1><a class="toc-backref" href="#id44">How to</a></h1>
<div class="section" id="how-to-add-a-new-node">
<h2><a class="toc-backref" href="#id45">How to add a new node</a></h2>
<p>Adding a new node to the corosync and pacemaker cluster will follow the steps listed above that describe installing the packages and configuring corosync.  Then, only start corosync.  If you are on the latest corosync/pacemaker version, you have two disctinct startup script it is easy to start only corosync.  If you are on an older version where only corosync is started, temporarily move the file <tt class="docutils literal">/etc/corosync/service.d/pacemaker</tt> to a safe place, like /root, and then start corosync.  That will cause the node to appear in the cluster when running <tt class="docutils literal">crm status</tt> on the old nodes.  Put the new node in standby with <tt class="docutils literal">crm node standby <span class="pre">host-09</span></tt> assuming the new node hostname is <tt class="docutils literal"><span class="pre">host-09</span></tt>.  Once in standby start pacemaker or for older installs, put the file <tt class="docutils literal">/etc/corosync/service.d/pacemaker</tt> back in place and restart corosync.</p>
<p>Once the new node has joined the cluster, you need to let the <tt class="docutils literal">ms</tt> resource know that it can have another clone (slave).  You can achieve this by increasing the <tt class="docutils literal"><span class="pre">clone-max</span></tt> attribute by one.</p>
<pre class="literal-block">
ms ms_MySQL p_mysql \
     meta master-max=&quot;1&quot; master-node-max=&quot;1&quot; clone-max=&quot;3&quot; clone-node-max=&quot;1&quot; notify=&quot;true&quot; globally-unique=&quot;false&quot; target-role=&quot;Master&quot; is-managed=&quot;true&quot;
</pre>
<p>Note that the easiest way to make this configuration change is with <tt class="docutils literal">crm configure edit</tt>, which allows you to edit the existing configuration in the EDITOR of your choice.  You may also want to put the pacemaker cluster into maintenance-mode first:</p>
<pre class="literal-block">
crm(live)configure# property maintenance-mode=on
crm(live)configure# commit
</pre>
<p>If the new node is added successfully to the existing corosync ring and pacemaker cluster, then it should appear in the <tt class="docutils literal">crm status</tt> and be in the <tt class="docutils literal">standby</tt> status.  Taking the cluster out of <tt class="docutils literal"><span class="pre">maintenance-mode</span></tt> should be safe at this point, but be sure to leave your new node in <tt class="docutils literal">standby</tt>.</p>
<p>Once the cluster is out of maintenance and the new node shows up in the configuration, you need to manually clone the new slave and set it up to replicate from whichever node is the active master.  This document will not cover the basics of cloning a slave.  Note that you will have to manually start mysql on your new node (be careful to do this exactly as pacemaker does it on the other nodes) once you have a full copy of the mysql data and before you execute your <tt class="docutils literal">CHANGE MASTER <span class="pre">...;</span> SLAVE START;</tt></p>
<p>Verify that the new node is working, replication is consistent, and allow it to catch up using standard methods.  Once it is caught up:</p>
<ol class="arabic simple">
<li>Shutdown the manually started mysql instance.  <tt class="docutils literal">mysqladmin shutdown</tt> may be helpful here.</li>
<li>Bring the node 'online' in pacemaker.  <tt class="docutils literal">crm node online new_node_name</tt></li>
</ol>
<p>The trick here is that PRM will not re-issue a CHANGE MASTER if it detects that the given mysql instance was already replicating from the current master node.  Once this node is online, then it should behave as other slave nodes and failover (and possibly be promoted to the master) accordingly.</p>
</div>
<div class="section" id="how-to-repair-replication">
<h2><a class="toc-backref" href="#id46">How to repair replication</a></h2>
<p>Repairing replication is an advanced mysql replication topic, which won't be covered in detail here.  However, it should be noted that there are two basic methods to repairing replication:</p>
<ol class="arabic simple">
<li>Inline repair (i.e., tools like <cite>pt-table-sync</cite>)</li>
<li>Repair by slave reclone (i.e., throw the slave's data away and re-clone it from the master or another slave )</li>
</ol>
<p>Inline repairs should not require any PRM intervention.  As far as PRM is concerned, it is all normal replication traffic.</p>
<p>Reclone repairs will end up following similar steps to the <tt class="docutils literal">How to add a new node</tt> steps above.  See above for details, but the basic steps are:</p>
<ol class="arabic simple">
<li>Put the offending slave into standby</li>
<li>Effect whatever repairs/data copying necessary</li>
<li>Bring the slave up manually, configure replication, and wait for it to catch up</li>
<li>Shutdown mysql on the slave</li>
<li>Bring the slave online in Pacemaker</li>
</ol>
</div>
<div class="section" id="how-to-exclude-a-node-from-the-master-role-or-less-likely-to-be">
<h2><a class="toc-backref" href="#id47">How to exclude a node from the master role (or less likely to be)</a></h2>
<p>Pacemaker offers a very powerful configuration language to do exactly this, and many variations are possible.   The simplest way is to simply assign a negative priority to the ms Master role and the node you want to exclude:</p>
<pre class="literal-block">
location avoid_being_the_master ms_MySQL \
        rule $role=&quot;Master&quot; -1000: #uname eq my_node
</pre>
<p>This should downgrade the possiblity of <tt class="docutils literal">my_node</tt> being the master unless there simply are no other candidates.  To prevent <tt class="docutils literal">my_node</tt> from becoming the master ever, simply take it further:</p>
<pre class="literal-block">
location never_be_the_master ms_MySQL \
        rule $role=&quot;Master&quot; -inf: #uname eq my_node
</pre>
</div>
<div class="section" id="how-to-verify-why-a-reader-vip-is-not-on-a-slave">
<h2><a class="toc-backref" href="#id48">How to verify why a reader VIP is not on a slave</a></h2>
<p>If there's enough reader VIPs for all slaves, the most likely cause is that the slave in question is not suitable for reads.  The best and quickest way to see if a slave is suitable to have a reader VIP is query the CIB like this:</p>
<pre class="literal-block">
root&#64;host-02:~# cibadmin -Q | grep readable | grep nvpair
       &lt;nvpair id=&quot;status-host-02-readable&quot; name=&quot;readable&quot; value=&quot;1&quot;/&gt;
       &lt;nvpair id=&quot;status-host-01-readable&quot; name=&quot;readable&quot; value=&quot;1&quot;/&gt;
</pre>
<p>This is the <tt class="docutils literal">readable</tt> attribute used in the location rules of the reader VIPs.  If the value is 0, there is something wrong with replication, either it is broken or lagging behind.</p>
</div>
<div class="section" id="how-to-clean-up-error-in-pacemaker">
<h2><a class="toc-backref" href="#id49">How to clean up error in pacemaker</a></h2>
<p>Pacemaker is rather verbose regarding errors (failed actions) it encounters and it the responsability of a human to acknowledge the errors but once acknowledge, how do you get rid of the error.  Here's an example error output from <tt class="docutils literal">crm status</tt>:</p>
<pre class="literal-block">
Online: [ pacemaker-1 pacemaker-2 ]

Master/Slave Set: ms_MySQL [p_mysql]
   Masters: [ pacemaker-2 ]
   Slaves: [ pacemaker-1 ]
reader_vip_1   (ocf::heartbeat:IPaddr2):       Started pacemaker-1
reader_vip_2   (ocf::heartbeat:IPaddr2):       Started pacemaker-2
writer_vip     (ocf::heartbeat:IPaddr2):       Started pacemaker-2

Failed actions:
   p_mysql:0_monitor_2000 (node=pacemaker-1, call=10, rc=1, status=complete): unknown error
</pre>
<p>Such failed actions are remove by this command:</p>
<pre class="literal-block">
crm resource cleanup p_mysql:0
</pre>
<p>where <tt class="docutils literal">p_mysql</tt> is the primitive name and <tt class="docutils literal">:0</tt> the clone set instance that has the error.</p>
</div>
<div class="section" id="configuring-a-report-slave-with-a-dedicated-vip">
<h2><a class="toc-backref" href="#id50">Configuring a report slave with a dedicated VIP</a></h2>
</div>
<div class="section" id="enabling-trace-in-the-resource-agent">
<h2><a class="toc-backref" href="#id51">Enabling trace in the resource agent</a></h2>
<p>The golden way of debugging a PRM setup is with the agent trace file which is the output of &quot;bash -x&quot;.  To enable the trace file simply do:</p>
<pre class="literal-block">
mkdir -p /tmp/mysql.ocf.ra.debug
touch /tmp/mysql.ocf.ra.debug/log
</pre>
<p>Be aware, this is a very chatty file, about 20MB/h.  If left unattented, it can fill a disk.  When you are done, simply remove the log file.
If you plan to keep it there, add a logrotate config file like:</p>
<pre class="literal-block">
[root&#64;host-01 mysql.ocf.ra.debug]# more /etc/logrotate.d/mysql-ra-trace
/tmp/mysql.ocf.ra.debug/log {
      # create 600 mysql mysql
      notifempty
      daily
      rotate 4
      missingok
      compress
   postrotate
      # just if mysqld is really running
      touch log
   endscript
}
</pre>
</div>
</div>
<div class="section" id="advanced-topics">
<h1><a class="toc-backref" href="#id52">Advanced topics</a></h1>
<div class="section" id="vipless-cluster-cloud">
<h2><a class="toc-backref" href="#id53">VIPless cluster (cloud)</a></h2>
<p>With many cloud provider, it is not possible to have virtual IPs so in that case, how can we reach the MySQL server.  For simplicity we'll consider only the master access, accessing the slaves for reads in such environment is possible but more challenging.  The principle of operation here will be to also run pacemaker on the application servers but instead of running MySQL, they'll be running a fake MySQL resource agent that will reconfigure access to the master based on the post-promote notification it will receive from the pacemaker cluster.  Configure the application with pacemaker like described above for a MySQL server but keep the node in standby for now.  Then, replace the mysql agent using the following procedure:</p>
<pre class="literal-block">
[root&#64;app-01 corosync]# cd /usr/lib/ocf/resource.d/
[root&#64;app-01 resource.d]# mkdir percona
[root&#64;app-01 resource.d]# cd percona/
[root&#64;app-01 percona]# wget -q -O mysql https://github.com/jayjanssen/Percona-Pacemaker-Resource-Agents/raw/master/fake_mysql_novip
[root&#64;app-01 percona]# chmod u+x mysql
</pre>
<p>By default the IP and port used are:</p>
<pre class="literal-block">
Fake_Master_IP=74.125.141.105  #a google IP
Fake_Master_port=3306
</pre>
<p>You must make sure your application use these values to connect to the master even though it is likely not the actual IP of the master server.  Next, we must change the configuration of Pacemaker in order to grow the master-slave clone set and prevent the master role from running on the application server node.  If initially we had 3 database nodes we would be replacing:</p>
<pre class="literal-block">
ms ms_MySQL p_mysql \
     meta master-max=&quot;1&quot; master-node-max=&quot;1&quot; clone-max=&quot;3&quot; \
     clone-node-max=&quot;1&quot; notify=&quot;true&quot; globally-unique=&quot;false&quot; \
     target-role=&quot;Master&quot; is-managed=&quot;true&quot;
</pre>
<p>with:</p>
<pre class="literal-block">
ms ms_MySQL p_mysql \
     meta master-max=&quot;1&quot; master-node-max=&quot;1&quot; clone-max=&quot;4&quot; \
     clone-node-max=&quot;1&quot; notify=&quot;true&quot; globally-unique=&quot;false&quot; \
     target-role=&quot;Master&quot; is-managed=&quot;true&quot;
location app_01_not_master ms_MySQL \
     rule $id=&quot;app_01_not_maste-rule&quot; $role=&quot;master&quot; -inf: #uname eq app-01
</pre>
<p>If you have many application servers, you can add them in a similar way.</p>
</div>
<div class="section" id="non-multicast-cluster-cloud">
<h2><a class="toc-backref" href="#id54">Non-multicast cluster (cloud)</a></h2>
<p>Cloud environment are also well known for their lack of support for Ethernet multicast (and broadcast).  There are 2 solutions to this problem, one using Heartbeat unicast and the other using Corosync udpu.  For Heartbeat, the ha.cf file will look like:</p>
<pre class="literal-block">
autojoin any
ucast eth0 10.1.1.1
ucast eth0 10.1.1.2
ucast eth0 10.1.1.3
warntime 5
deadtime 15
initdead 60
keepalive 2
crm respawn
</pre>
<p>and for corosync, the corosync.conf file with the udpu configuration looks like:</p>
<pre class="literal-block">
compatibility: whitetank

totem {
      version: 2
      secauth: on
      threads: 0
      interface {
               member {
                        memberaddr: 10.1.1.1
               }
               member {
                        memberaddr: 10.1.1.2
               }
               member {
                        memberaddr: 10.1.1.3
               }
               ringnumber: 0
               bindnetaddr: 10.1.1.0
               netmask: 255.255.255.0
               mcastport: 5405
               ttl: 1
      }
         transport: udpu
}

logging {
      fileline: off
      to_stderr: no
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
               subsys: AMF
               debug: off
      }
}

amf {
      mode: disabled
}
</pre>
<p>Be aware that in order to use <tt class="docutils literal">udpu</tt> with corosync, you need version 1.3+.</p>
</div>
<div class="section" id="stonith-devices">
<h2><a class="toc-backref" href="#id55">Stonith devices</a></h2>
<p>An HA setup without stonith devices is relying on the willingness of the nodes to perform the required tasks.  When everything is running fine, there's no problem to make such an assumption but if you are considering HA, it is because you want to cover cases where things are going wrong.  For example, take one of the simplest HA resource, a VIP.  In order to create and remove the VIP, Pacemaker needs to access the <tt class="docutils literal">/sbin/ip</tt> binary.  What happends if the filesystem is not available?  The kernel has the VIP defined but Pacemaker is unable to remove it.  Another node in the cluster will start the VIP and boom... you have twice the same IP on your network.  So, you need a way to resolve cases when a node cannot perform a required task like releasing a resource.  Fencing is answer and stonith (Shoot The Other Node In The Head) devices are the implementation.  There are many stonith devices available but the most commons are IPMI and ILO.  To get access to the most recent stonith devices, install the package <tt class="docutils literal"><span class="pre">fence-agents</span></tt> from RedHat cluster, these are usable with Pacemaker.  In pacemaker, stonith devices are defined a bit like normal primitives.  Here's an example using ILO:</p>
<pre class="literal-block">
primitive stonith-host-01 stonith:fence_ilo \
      params pcmk_host_list=&quot;host-01&quot; pcmk_host_check=&quot;static-list&quot; \
      ipaddr=&quot;10.1.2.1&quot; login=&quot;iloadmin&quot; passwd=&quot;ilopass&quot; verbose=&quot;true&quot; \
      op monitor interval=&quot;60s&quot;
primitive stonith-host-02 stonith:fence_ilo \
      params pcmk_host_list=&quot;host-02&quot; pcmk_host_check=&quot;static-list&quot; \
      ipaddr=&quot;10.1.2.2&quot; login=&quot;iloadmin&quot; passwd=&quot;ilopass&quot; verbose=&quot;true&quot; \
      op monitor interval=&quot;60s&quot;
location stonith-host-01_loc stonith-host-01 \
      rule $id=&quot;stonith-host-01_loc-rule&quot; -inf: #uname eq host-01
location stonith-host-02_loc stonith-host-02 \
      rule $id=&quot;stonith-host-02_loc-rule&quot; -inf: #uname eq host-02
</pre>
<p>In the above example, IPs in the 10.1.2.x are the IPs of the ILO devices.  For each ILO device, you specify in the pcmk_host_list which host it fences. We also need location rules to prevent a stonith device to run on the node it is supposed to kill.</p>
</div>
<div class="section" id="using-heartbeat">
<h2><a class="toc-backref" href="#id56">Using heartbeat</a></h2>
<p>Although Corosync is now the default communication stack with Pacemaker, Pacemaker works also well with Hearbeat. Here are the steps you need to configure Heartbeat instead of Corosync.  The first thing, you need a cluster key which can be created as simply as:</p>
<pre class="literal-block">
echo 'auth 1' &gt; /etc/ha.d/authkeys
echo -n '1 sha1 ' &gt;&gt; /etc/ha.d/authkeys
date | md5sum &gt;&gt; /etc/ha.d/authkeys
chown root.root /etc/ha.d/authkeys
chmod 600 /etc/ha.d/authkeys
</pre>
<p>Copy this file to all the nodes and preserve the ownership and rights.  Then, we must configure heartbeat to use pacemaker.  Here's a very simple Heartbeat configuration file (/etc/ha.d/ha.cf):</p>
<pre class="literal-block">
autojoin any
bcast eth0
warntime 5
deadtime 15
initdead 60
keepalive 2
crm respawn
</pre>
<p>Any node with the right authkeys file will be able to join (autojoin any).  Communication will be using ethernet broadcast (bcast) but multicast or even unicast could also be used.  Finally, Pacemaker is started with the &quot;crm respawn&quot; line.  Compared to the corosync setup described above, in order to start Pacemaker with Heartbeat, you just need to start Heartbeat.</p>
</div>
<div class="section" id="performing-rolling-restarts-for-config-changes">
<h2><a class="toc-backref" href="#id57">Performing rolling restarts for config changes</a></h2>
<p>Because failover is automated on the PRM cluster, performing rolling configuration changes that require mysql restart (i.e., not dynamic variables) is fairly straightforward:</p>
<ol class="arabic simple">
<li>Set the node to standby</li>
<li>Make configuration changes</li>
<li>Set the node to online</li>
<li>Go to the next node</li>
</ol>
</div>
<div class="section" id="backups-with-prm">
<h2><a class="toc-backref" href="#id58">Backups with PRM</a></h2>
<p>There are a few basic ways to take a mysql backup, so depending on your method it will affect what steps you need to take in pacemaker (if any).</p>
<p>If MySQL can continue running and the load of the backup is not a problem for continuing service on the slave, then you don't need to do anything.  Simply take your backup and allow normal service to continue.</p>
<p>If you need to shift production traffic away from the node (i.e., a reader vip), then simply move the resource to some other node:</p>
<pre class="literal-block">
crm move slave_vip_running_on_backup_node not_the_backup_node
</pre>
<p>Perform your backup here (note replication will remain running, but tools like mysqldump should not have a problem with this because it either locks the tables or wraps its backup in a transaction).  Then, to allow pacemaker to resume management of that vip:</p>
<pre class="literal-block">
crm unmove the_slave_vip_you_moved
</pre>
<p>If you need to fully shutdown mysql to take your backup, it's best to simply standby the node:</p>
<pre class="literal-block">
crm node standby backup_node
</pre>
<p><em>further topics</em>:</p>
<ul class="simple">
<li>Determining good backup candidate (i.e., not the master)</li>
<li>Prohibiting the selected backup node from being eligible for the master during the backup.</li>
<li>Using Xtrabackup's --safe-slave-backup with a PRM slave (see <a class="reference external" href="https://github.com/jayjanssen/Percona-Pacemaker-Resource-Agents/issues/3">Issue Here</a>)</li>
</ul>
</div>
</div>
</div>
</body>
</html>
